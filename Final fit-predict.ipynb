{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a140480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_GPU = False\n",
    "# Импорт нужных библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "from neighbors import Neighborhoods\n",
    "\n",
    "from indices import MainDataset\n",
    "from dnn_utils import preprocess_floor\n",
    "from metric import metrics_stat, deviation_metric\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "\n",
    "def reset_tensorflow_session():\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(41)\n",
    "    np.random.seed(41)\n",
    "\n",
    "\n",
    "THRESHOLD = 0.15\n",
    "NEGATIVE_WEIGHT = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73007644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Категориальные данные\n",
    "CATEGORICAL_FEATURES_COLUMNS = ['region', 'city', 'realty_type', 'floor', 'osm_city_nearest_name', 'street']\n",
    "# Численные данные\n",
    "NUM_FEATURES_COLUMNS = ['lat', 'lng', 'osm_amenity_points_in_0.001',\n",
    "                        'osm_amenity_points_in_0.005', 'osm_amenity_points_in_0.0075',\n",
    "                        'osm_amenity_points_in_0.01', 'osm_building_points_in_0.001',\n",
    "                        'osm_building_points_in_0.005', 'osm_building_points_in_0.0075',\n",
    "                        'osm_building_points_in_0.01', 'osm_catering_points_in_0.001',\n",
    "                        'osm_catering_points_in_0.005', 'osm_catering_points_in_0.0075',\n",
    "                        'osm_catering_points_in_0.01', 'osm_city_closest_dist',\n",
    "                        'osm_city_nearest_population',\n",
    "                        'osm_crossing_closest_dist', 'osm_crossing_points_in_0.001',\n",
    "                        'osm_crossing_points_in_0.005', 'osm_crossing_points_in_0.0075',\n",
    "                        'osm_crossing_points_in_0.01', 'osm_culture_points_in_0.001',\n",
    "                        'osm_culture_points_in_0.005', 'osm_culture_points_in_0.0075',\n",
    "                        'osm_culture_points_in_0.01', 'osm_finance_points_in_0.001',\n",
    "                        'osm_finance_points_in_0.005', 'osm_finance_points_in_0.0075',\n",
    "                        'osm_finance_points_in_0.01', 'osm_healthcare_points_in_0.005',\n",
    "                        'osm_healthcare_points_in_0.0075', 'osm_healthcare_points_in_0.01',\n",
    "                        'osm_historic_points_in_0.005', 'osm_historic_points_in_0.0075',\n",
    "                        'osm_historic_points_in_0.01', 'osm_hotels_points_in_0.005',\n",
    "                        'osm_hotels_points_in_0.0075', 'osm_hotels_points_in_0.01',\n",
    "                        'osm_leisure_points_in_0.005', 'osm_leisure_points_in_0.0075',\n",
    "                        'osm_leisure_points_in_0.01', 'osm_offices_points_in_0.001',\n",
    "                        'osm_offices_points_in_0.005', 'osm_offices_points_in_0.0075',\n",
    "                        'osm_offices_points_in_0.01', 'osm_shops_points_in_0.001',\n",
    "                        'osm_shops_points_in_0.005', 'osm_shops_points_in_0.0075',\n",
    "                        'osm_shops_points_in_0.01', 'osm_subway_closest_dist',\n",
    "                        'osm_train_stop_closest_dist', 'osm_train_stop_points_in_0.005',\n",
    "                        'osm_train_stop_points_in_0.0075', 'osm_train_stop_points_in_0.01',\n",
    "                        'osm_transport_stop_closest_dist', 'osm_transport_stop_points_in_0.005',\n",
    "                        'osm_transport_stop_points_in_0.0075',\n",
    "                        'osm_transport_stop_points_in_0.01',\n",
    "                        'reform_count_of_houses_1000', 'reform_count_of_houses_500',\n",
    "                        'reform_house_population_1000', 'reform_house_population_500',\n",
    "                        'reform_mean_floor_count_1000', 'reform_mean_floor_count_500',\n",
    "                        'reform_mean_year_building_1000', 'reform_mean_year_building_500', 'total_square',\n",
    "                        \"neighbor_dist\", \"neighbor_total_price\", \"neighbor_square_price\", \"neighbor10_dist\",\n",
    "                        \"has_basement\", \"floor_count\"\n",
    "\n",
    "                        ]\n",
    "# Таргет\n",
    "TARGET_COLUMNS = ['per_square_meter_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a970f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "train = train[train.price_type == 1].reset_index(drop=True)\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "dataset = pd.concat([train, test]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d49f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_index = MainDataset(\"dataset/train.csv\")\n",
    "test_dataset_index = MainDataset(\"dataset/test.csv\", need_index=False)\n",
    "neighborhoods = Neighborhoods(train_dataset_index.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"neighbor_dist\"] = -999\n",
    "dataset[\"neighbor_total_price\"] = -999\n",
    "dataset[\"neighbor_square_price\"] = -999\n",
    "dataset[\"neighbor10_dist\"] = -999\n",
    "for d in [test_dataset_index, train_dataset_index]:\n",
    "    for i, o in enumerate(d.all_objects):\n",
    "        if o.row[\"price_type\"] != 1:\n",
    "            continue\n",
    "        neighbor = neighborhoods.get_haversine_closest(o, 12)\n",
    "        neighbor1 = neighborhoods.get_haversine_closest(o, 2)\n",
    "        n = neighbor[0]\n",
    "        dataset.loc[dataset[\"id\"] == o.row[\"id\"], \"neighbor_dist\"] = n[1]\n",
    "        dataset.loc[dataset[\"id\"] == o.row[\"id\"], \"neighbor_total_price\"] = n[0].row[\"per_square_meter_price\"] * \\\n",
    "                                                                            n[0].row[\"total_square\"]\n",
    "        dataset.loc[dataset[\"id\"] == o.row[\"id\"], \"neighbor_square_price\"] = n[0].row[\"per_square_meter_price\"]\n",
    "        dataset.loc[dataset[\"id\"] == o.row[\"id\"], \"neighbor10_dist\"] = neighbor[10][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b660cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset=preprocess_floor.preprocess(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58922f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(df, categorical_columns):\n",
    "    for column in categorical_columns:\n",
    "        dict_encoding = {key: val for val, key in enumerate(df[column].unique())}\n",
    "        df[column] = df[column].map(dict_encoding)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encode_categorical_features(dataset, CATEGORICAL_FEATURES_COLUMNS)\n",
    "data = data.fillna(data.mean())\n",
    "train = data[data.is_train == 1].reset_index(drop=True)\n",
    "test = data[data.is_train == 0].reset_index(drop=True)\n",
    "train = train.drop(columns=['is_train'])\n",
    "test = test.drop(columns=['is_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e665cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standart_split(data, n_splits=5, seed=41):\n",
    "    kf = KFold(n_splits=n_splits, random_state=seed, shuffle=True)\n",
    "    split_list = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        split_list += [(train_index, test_index)]\n",
    "    return split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ad8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_order(columns):\n",
    "    columns_order = sorted([x for x in columns if not x in (CATEGORICAL_FEATURES_COLUMNS + TARGET_COLUMNS)])\n",
    "    return columns_order + CATEGORICAL_FEATURES_COLUMNS + TARGET_COLUMNS\n",
    "\n",
    "features_columns_order = get_columns_order(train.columns.values.tolist())\n",
    "\n",
    "split_list = get_standart_split(train, n_splits=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa189b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xbg_error(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    err = deviation_metric(np.exp(labels), np.exp(preds)/1.1)\n",
    "    return 'deviation_error', err\n",
    "\n",
    "\n",
    "def train_xgb(train, valid, num_features, categorical_features, target_train, target_valid, EPOCHS, params):\n",
    "    dtest = xgb.DMatrix(test[num_features + categorical_features])\n",
    "    y_valid = np.zeros(len(valid))\n",
    "\n",
    "    dtrain = xgb.DMatrix(train[num_features + categorical_features], np.log(target_train), \n",
    "                        )\n",
    "    dvalid = xgb.DMatrix(valid[num_features + categorical_features], np.log(target_valid), \n",
    "                        )\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        EPOCHS,\n",
    "        [(dvalid, \"valid\")],\n",
    "        verbose_eval=250,\n",
    "        early_stopping_rounds=500,\n",
    "        feval=xbg_error,\n",
    "    )\n",
    "    y_valid = model.predict(dvalid)\n",
    "\n",
    "    return model, y_valid\n",
    "\n",
    "\n",
    "start_train_model_time = time.time()\n",
    "\n",
    "xgboost_seed = 41\n",
    "xgboost_params = {\n",
    "    \"subsample\": 0.70,\n",
    "    \"colsample_bytree\": 0.50,\n",
    "    \"max_depth\": 7,\n",
    "    \"learning_rate\": 0.012,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    'disable_default_eval_metric': 1,\n",
    "    \"nthread\": -1,\n",
    "    \"max_bin\": 128,\n",
    "    'min_child_weight': 0.0,\n",
    "    'reg_lambda': 0.0,\n",
    "    'reg_alpha': 0.0,\n",
    "    'seed': xgboost_seed,\n",
    "}\n",
    "\n",
    "\n",
    "EPOCHS = 10000\n",
    "scores = []\n",
    "xgb_predicts = np.zeros(len(train))\n",
    "\n",
    "xgb_models = []\n",
    "for fold_num, (train_indexes, valid_indexes) in enumerate(split_list):\n",
    "    start_time = time.time()\n",
    "    print(f\"Фолд: {fold_num}\")\n",
    "\n",
    "    train_sub_df = train[features_columns_order].loc[train_indexes].reset_index(drop=True)\n",
    "    valid_sub_df = train[features_columns_order].loc[valid_indexes].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Размер трейна = {train_sub_df.shape} Размер валидации = {valid_sub_df.shape}\")\n",
    "    # Обучаем Xgboost и делаем предикт на валидационной выборке\n",
    "    model, predict_validation = train_xgb(\n",
    "        train_sub_df,\n",
    "        valid_sub_df,\n",
    "        NUM_FEATURES_COLUMNS,\n",
    "        CATEGORICAL_FEATURES_COLUMNS,\n",
    "        train_sub_df[TARGET_COLUMNS[0]].values,\n",
    "        valid_sub_df[TARGET_COLUMNS[0]].values,\n",
    "        EPOCHS,\n",
    "        xgboost_params)\n",
    "\n",
    "    xgb_models += [model]\n",
    "    predict_on_validation = model.predict(\n",
    "        xgb.DMatrix(valid_sub_df[NUM_FEATURES_COLUMNS + CATEGORICAL_FEATURES_COLUMNS]))\n",
    "    xgb_predicts[valid_indexes] = np.exp(predict_on_validation)\n",
    "    targets_for_validation = valid_sub_df[TARGET_COLUMNS].values[:, 0]\n",
    "    current_score = deviation_metric(targets_for_validation, predict_on_validation)\n",
    "    scores += [current_score]\n",
    "    print(\n",
    "        f\"Скор для фолда({fold_num}) : {np.round(current_score, 4)} средний скор на префиксе = {np.round(np.mean(scores), 4)} это заняло = {int(time.time() - start_time)} сек.\")\n",
    "print(f\"Процесс обучения модели занял = {int(time.time() - start_train_model_time)} секунд\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b0cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предикт xgb на test\n",
    "def get_xgb_predict(models, test):\n",
    "    result = np.zeros(len(test))\n",
    "    for model in models:\n",
    "        predict = model.predict(xgb.DMatrix(test[NUM_FEATURES_COLUMNS + CATEGORICAL_FEATURES_COLUMNS]))\n",
    "        result += predict / len(models)\n",
    "    return result\n",
    "\n",
    "test_xgb_predict = get_xgb_predict(xgb_models, test)\n",
    "\n",
    "test_xgb_predict=np.exp(test_xgb_predict)\n",
    "\n",
    "test_xgb_predict.min(), test_xgb_predict.max(), test_xgb_predict.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ba129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ca116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostEvalMetricPearson(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return error\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return False\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "        preds = np.array(approxes[0])\n",
    "        target = np.array(target)\n",
    "        err = deviation_metric(np.exp(target), np.exp(preds)/1.1)\n",
    "        return err, 0\n",
    "\n",
    "\n",
    "def train_cat(train, valid, num_features, categorical_features, target_train, target_valid, EPOCHS):\n",
    "\n",
    "    test_data = Pool(data=test[num_features + categorical_features],\n",
    "                  cat_features=categorical_features)\n",
    "\n",
    "\n",
    "    train_data = Pool(data=train[num_features + categorical_features],\n",
    "                      cat_features=categorical_features,\n",
    "                      label=np.log(target_train))\n",
    "\n",
    "    val_data = Pool(data=valid[num_features + categorical_features],\n",
    "                cat_features=categorical_features,\n",
    "                  label=np.log(target_valid))\n",
    "\n",
    "    cat_model = CatBoostRegressor(\n",
    "        l2_leaf_reg=6,\n",
    "        bagging_temperature=1.3,\n",
    "        random_strength=1.2,\n",
    "        learning_rate=0.02,\n",
    "        iterations=10000,\n",
    "        metric_period=50,\n",
    "        eval_metric=CatBoostEvalMetricPearson(),\n",
    "    )\n",
    "    cat_model.fit(train_data, eval_set=val_data, use_best_model=True, early_stopping_rounds=300)\n",
    "  \n",
    "    y_valid = cat_model.predict(test_data)\n",
    "\n",
    "    return cat_model, y_valid\n",
    "\n",
    "\n",
    "start_train_model_time = time.time()\n",
    "\n",
    "scores = []\n",
    "cat_predicts = np.zeros(len(train))\n",
    "\n",
    "cat_models = []\n",
    "for fold_num, (train_indexes, valid_indexes) in enumerate(split_list):\n",
    "    start_time = time.time()\n",
    "    print(f\"Фолд: {fold_num}\")\n",
    "\n",
    "    train_sub_df = train[features_columns_order].loc[train_indexes].reset_index(drop=True)\n",
    "    valid_sub_df = train[features_columns_order].loc[valid_indexes].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Размер трейна = {train_sub_df.shape} Размер валидации = {valid_sub_df.shape}\")\n",
    "    model, predict_validation = train_cat(\n",
    "        train_sub_df,\n",
    "        valid_sub_df,\n",
    "        NUM_FEATURES_COLUMNS,\n",
    "        CATEGORICAL_FEATURES_COLUMNS,\n",
    "        train_sub_df[TARGET_COLUMNS[0]].values,\n",
    "        valid_sub_df[TARGET_COLUMNS[0]].values,\n",
    "        EPOCHS\n",
    "        )\n",
    "\n",
    "    cat_models += [model]\n",
    "    predict_on_validation = model.predict(valid_sub_df[NUM_FEATURES_COLUMNS + CATEGORICAL_FEATURES_COLUMNS])\n",
    "    cat_predicts[valid_indexes] = np.exp(predict_on_validation)\n",
    "    targets_for_validation = valid_sub_df[TARGET_COLUMNS].values[:, 0]\n",
    "    current_score = deviation_metric(targets_for_validation, predict_on_validation)\n",
    "    scores += [current_score]\n",
    "    print(\n",
    "        f\"Скор для фолда({fold_num}) : {np.round(current_score, 4)} средний скор на префиксе = {np.round(np.mean(scores), 4)} это заняло = {int(time.time() - start_time)} сек.\")\n",
    "print(f\"Процесс обучения модели занял = {int(time.time() - start_train_model_time)} секунд\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_cat_predict(models, test):\n",
    "    result = np.zeros(len(test))\n",
    "    for model in cat_models:\n",
    "        predict = model.predict(test[NUM_FEATURES_COLUMNS + CATEGORICAL_FEATURES_COLUMNS])\n",
    "        result += np.exp(predict) / len(models)\n",
    "    return result\n",
    "\n",
    "\n",
    "test_cat_predict = get_cat_predict(xgb_models, test)\n",
    "\n",
    "test_cat_predict.min(), test_cat_predict.max(), test_cat_predict.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f5a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1f5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce25f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = train[TARGET_COLUMNS[0]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82315547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_arit(W):\n",
    "    ypred =  W[0] * xgb_predicts + W[1] * cat_predicts\n",
    "    return deviation_metric(train_targets, ypred)\n",
    "\n",
    "\n",
    "W = minimize(minimize_arit, [1.0 / 2] * 2, options={'gtol': 1e-6, 'disp': True}).x\n",
    "W\n",
    "# 1.006250\n",
    "# array([0.55692824, 0.34630855])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_submission = pd.read_csv('dataset/test_submission.csv')\n",
    "test_submission['per_square_meter_price'] = test_xgb_predict * W[0] + test_cat_predict * W[1]\n",
    "test_submission['per_square_meter_price'] = test_submission['per_square_meter_price'].apply(lambda x: max(1000.0, x))\n",
    "test_submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4e599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "344c167b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_predicts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-66e00d8654b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mxgb_predicts_spb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_predicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcity_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcat_predicts_spb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_predicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcity_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnn_predicts_spb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_predicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcity_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlgb_predicts_spb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_predicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcity_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_targets_spb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcity_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_predicts' is not defined"
     ]
    }
   ],
   "source": [
    "for city_id in [0,4,2,22]:\n",
    "    xgb_predicts_spb = xgb_predicts[train.city == city_id]\n",
    "    cat_predicts_spb = cat_predicts[train.city == city_id]\n",
    "    train_targets_spb = train_targets[train.city == city_id]\n",
    "\n",
    "    test_xgb_predict_spb = test_xgb_predict[test.city == city_id]\n",
    "    test_cat_predict_spb = test_cat_predict[test.city == city_id]\n",
    "\n",
    "    def minimize_arit_spb(W):\n",
    "        ypred = W[0] * xgb_predicts_spb + W[1] * cat_predicts_spb\n",
    "        return deviation_metric(train_targets_spb, ypred)\n",
    "\n",
    "\n",
    "    W_spb = minimize(minimize_arit_spb, [1.0 / 2] * 2, options={'gtol': 1e-6, 'disp': True}).x\n",
    "    print(W_spb)\n",
    "    new_score = test_xgb_predict_spb * W_spb[0] + test_cat_predict_spb * W_spb[1]\n",
    "    old_score = test_submission.loc[test.city == city_id, 'per_square_meter_price'].values\n",
    "    test_submission.loc[test.city == city_id, 'per_square_meter_price'] =  (new_score+old_score)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3550cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_submission['per_square_meter_price'] = test_xgb_predict * W[0] + test_cat_predict * W[1]\n",
    "test_submission['per_square_meter_price'] = test_submission['per_square_meter_price'].apply(lambda x: max(1000.0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ea29a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.239194\n",
      "         Iterations: 9\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 11\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.299170\n",
      "         Iterations: 8\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 10\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.992637\n",
      "         Iterations: 9\n",
      "         Function evaluations: 36\n",
      "         Gradient evaluations: 12\n"
     ]
    }
   ],
   "source": [
    "for realty_id in [0,1,2]:\n",
    "    xgb_predicts_spb = xgb_predicts[train.realty_type == realty_id]\n",
    "    cat_predicts_spb = cat_predicts[train.realty_type == realty_id]\n",
    "    train_targets_spb = train_targets[train.realty_type == realty_id]\n",
    "\n",
    "    test_xgb_predict_spb = test_xgb_predict[test.realty_type == realty_id]\n",
    "    test_cat_predict_spb = test_cat_predict[test.realty_type == realty_id]\n",
    "\n",
    "    def minimize_arit_spb(W):\n",
    "        ypred = W[0] * xgb_predicts_spb + W[1] * cat_predicts_spb\n",
    "        return deviation_metric(train_targets_spb, ypred)\n",
    "\n",
    "\n",
    "    W_spb = minimize(minimize_arit_spb, [1.0 / 2] * 2, options={'gtol': 1e-6, 'disp': True}).x\n",
    "\n",
    "    new_score = test_xgb_predict_spb * W_spb[0] + test_cat_predict_spb * W_spb[1]\n",
    "    old_score = test_submission.loc[test.realty_type == realty_id, 'per_square_meter_price'].values\n",
    "    test_submission.loc[test.realty_type == realty_id, 'per_square_meter_price'] =  (new_score+old_score)/2\n",
    "\n",
    "test_submission.to_csv('submission_realty.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a4995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6efc1611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5435d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
