{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1c69b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_GPU = False\n",
    "# Импорт нужных библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "from neighbors import Neighborhoods\n",
    "\n",
    "from indices import MainDataset\n",
    "from dnn_utils import preprocess_floor\n",
    "from metric import metrics_stat, deviation_metric\n",
    "\n",
    "def reset_tensorflow_session():\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(41)\n",
    "    np.random.seed(41)\n",
    "\n",
    "\n",
    "THRESHOLD = 0.15\n",
    "NEGATIVE_WEIGHT = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad81492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Категориальные данные\n",
    "CATEGORICAL_FEATURES_COLUMNS = ['region', 'city', 'realty_type', 'floor', 'osm_city_nearest_name', 'street']\n",
    "# Численные данные\n",
    "NUM_FEATURES_COLUMNS = ['lat', 'lng', 'osm_amenity_points_in_0.001',\n",
    "                        'osm_amenity_points_in_0.005', 'osm_amenity_points_in_0.0075',\n",
    "                        'osm_amenity_points_in_0.01', 'osm_building_points_in_0.001',\n",
    "                        'osm_building_points_in_0.005', 'osm_building_points_in_0.0075',\n",
    "                        'osm_building_points_in_0.01', 'osm_catering_points_in_0.001',\n",
    "                        'osm_catering_points_in_0.005', 'osm_catering_points_in_0.0075',\n",
    "                        'osm_catering_points_in_0.01', 'osm_city_closest_dist',\n",
    "                        'osm_city_nearest_population',\n",
    "                        'osm_crossing_closest_dist', 'osm_crossing_points_in_0.001',\n",
    "                        'osm_crossing_points_in_0.005', 'osm_crossing_points_in_0.0075',\n",
    "                        'osm_crossing_points_in_0.01', 'osm_culture_points_in_0.001',\n",
    "                        'osm_culture_points_in_0.005', 'osm_culture_points_in_0.0075',\n",
    "                        'osm_culture_points_in_0.01', 'osm_finance_points_in_0.001',\n",
    "                        'osm_finance_points_in_0.005', 'osm_finance_points_in_0.0075',\n",
    "                        'osm_finance_points_in_0.01', 'osm_healthcare_points_in_0.005',\n",
    "                        'osm_healthcare_points_in_0.0075', 'osm_healthcare_points_in_0.01',\n",
    "                        'osm_historic_points_in_0.005', 'osm_historic_points_in_0.0075',\n",
    "                        'osm_historic_points_in_0.01', 'osm_hotels_points_in_0.005',\n",
    "                        'osm_hotels_points_in_0.0075', 'osm_hotels_points_in_0.01',\n",
    "                        'osm_leisure_points_in_0.005', 'osm_leisure_points_in_0.0075',\n",
    "                        'osm_leisure_points_in_0.01', 'osm_offices_points_in_0.001',\n",
    "                        'osm_offices_points_in_0.005', 'osm_offices_points_in_0.0075',\n",
    "                        'osm_offices_points_in_0.01', 'osm_shops_points_in_0.001',\n",
    "                        'osm_shops_points_in_0.005', 'osm_shops_points_in_0.0075',\n",
    "                        'osm_shops_points_in_0.01', 'osm_subway_closest_dist',\n",
    "                        'osm_train_stop_closest_dist', 'osm_train_stop_points_in_0.005',\n",
    "                        'osm_train_stop_points_in_0.0075', 'osm_train_stop_points_in_0.01',\n",
    "                        'osm_transport_stop_closest_dist', 'osm_transport_stop_points_in_0.005',\n",
    "                        'osm_transport_stop_points_in_0.0075',\n",
    "                        'osm_transport_stop_points_in_0.01',\n",
    "                        'reform_count_of_houses_1000', 'reform_count_of_houses_500',\n",
    "                        'reform_house_population_1000', 'reform_house_population_500',\n",
    "                        'reform_mean_floor_count_1000', 'reform_mean_floor_count_500',\n",
    "                        'reform_mean_year_building_1000', 'reform_mean_year_building_500', 'total_square',\n",
    "                        \"neighbor_dist\", \"neighbor_total_price\", \"neighbor_square_price\", \"neighbor10_dist\",\n",
    "                        \"has_basement\", \"floor_count\"\n",
    "\n",
    "                        ]\n",
    "# Таргет\n",
    "TARGET_COLUMNS = ['per_square_meter_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f38513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "train = train[train.price_type == 1].reset_index(drop=True)\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "dataset = pd.concat([train, test]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_index = MainDataset(\"dataset/train.csv\")\n",
    "test_dataset_index = MainDataset(\"dataset/test.csv\", need_index=False)\n",
    "neighborhoods = Neighborhoods(train_dataset_index.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"neighbor_dist\"] = -999\n",
    "dataset[\"neighbor_total_price\"] = -999\n",
    "dataset[\"neighbor_square_price\"] = -999\n",
    "dataset[\"neighbor10_dist\"] = -999\n",
    "for d in [test_dataset_index, train_dataset_index]:\n",
    "    for i, o in enumerate(d.all_objects):\n",
    "        if o.row[\"price_type\"] != 1:\n",
    "            continue\n",
    "        neighbor = neighborhoods.get_haversine_closest(o, 12)\n",
    "        neighbor1 = neighborhoods.get_haversine_closest(o, 2)\n",
    "        n = neighbor[0]\n",
    "        dataset.loc[dataset[\"id\"] == o.row[\"id\"], \"neighbor_dist\"] = n[1]\n",
    "        dataset.loc[dataset[\"id\"] == o.row[\"id\"], \"neighbor_total_price\"] = n[0].row[\"per_square_meter_price\"] * \\\n",
    "                                                                            n[0].row[\"total_square\"]\n",
    "        dataset.loc[dataset[\"id\"] == o.row[\"id\"], \"neighbor_square_price\"] = n[0].row[\"per_square_meter_price\"]\n",
    "        dataset.loc[dataset[\"id\"] == o.row[\"id\"], \"neighbor10_dist\"] = neighbor[10][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d686ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset=preprocess_floor.preprocess(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe00571",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(df, categorical_columns):\n",
    "    for column in categorical_columns:\n",
    "        dict_encoding = {key: val for val, key in enumerate(df[column].unique())}\n",
    "        df[column] = df[column].map(dict_encoding)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Квантильное преобразование данных\n",
    "def get_quantile_transform(_df, columns_for_quantilization, random_state=41, n_quantiles=100,\n",
    "                           output_distribution='normal'):\n",
    "    df = _df.copy()\n",
    "    for col in columns_for_quantilization:\n",
    "        qt = QuantileTransformer(random_state=random_state, n_quantiles=n_quantiles,\n",
    "                                 output_distribution=output_distribution)\n",
    "        df[col] = qt.fit_transform(df[[col]])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# МинМакс преобразование данных\n",
    "def get_minmax_transform(_df, columns_for_quantilization, min_value=-1, max_value=1):\n",
    "    df = _df.copy()\n",
    "    for col in columns_for_quantilization:\n",
    "        scaler = MinMaxScaler(feature_range=(min_value, max_value))\n",
    "        df[col] = scaler.fit_transform(df[[col]])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hotencoding для категориальных фичей\n",
    "data = encode_categorical_features(dataset, CATEGORICAL_FEATURES_COLUMNS)\n",
    "# Нормализация численных данных\n",
    "data = get_quantile_transform(data, NUM_FEATURES_COLUMNS)\n",
    "data = get_minmax_transform(data, NUM_FEATURES_COLUMNS)\n",
    "# Заполняем NaN значения\n",
    "data = data.fillna(data.mean())\n",
    "train = data[data.is_train == 1].reset_index(drop=True)\n",
    "test = data[data.is_train == 0].reset_index(drop=True)\n",
    "train = train.drop(columns=['is_train'])\n",
    "test = test.drop(columns=['is_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda13b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standart_split(data, n_splits=5, seed=41):\n",
    "    kf = KFold(n_splits=n_splits, random_state=seed, shuffle=True)\n",
    "    split_list = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        split_list += [(train_index, test_index)]\n",
    "    return split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58abc8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(arr_features, arr_target, arr_region, arr_city, arr_realty, batch_size):\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "                \"model_features_input\": arr_features,\n",
    "                \"model_region_input\": arr_region,\n",
    "                \"model_city_input\": arr_city,\n",
    "                \"model_realty_input\": arr_realty,\n",
    "            },\n",
    "            {\n",
    "                \"model_output\": arr_target,\n",
    "            },\n",
    "        )\n",
    "    ).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_order(columns):\n",
    "    columns_order = sorted([x for x in columns if not x in (CATEGORICAL_FEATURES_COLUMNS + TARGET_COLUMNS)])\n",
    "    return columns_order + CATEGORICAL_FEATURES_COLUMNS + TARGET_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Коллбэк, для отслеживания целевой метрики\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, val_dataset, val_targets):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.val_targets = val_targets\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        predicts = self.model.predict(self.val_dataset)[:, 0]\n",
    "        targets = self.val_targets[:, 0]\n",
    "        print(f\"Текущий реальный скор(валидационная часть): {np.round(deviation_metric(targets, predicts), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10647aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Dropout(x):\n",
    "    return keras.layers.Dropout(x)\n",
    "\n",
    "\n",
    "def Flatten():\n",
    "    return keras.layers.Flatten()\n",
    "\n",
    "\n",
    "def Concatenate():\n",
    "    return keras.layers.Concatenate()\n",
    "\n",
    "\n",
    "# Функция обучения модели\n",
    "def fit(model, epochs, train_dataset, val_dataset, val_targets, verbose=True):\n",
    "    if IS_GPU:\n",
    "        print(f\"Начинаю обучение модели (GPU) количество эпох = {epochs}\")\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            # Коллбэк для остановки, если модель перестала обучаться\n",
    "            early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=2.5e-6,\n",
    "                                                                       patience=100, restore_best_weights=True,\n",
    "                                                                       mode='min')\n",
    "            # Коллбэк для уменьшения скорости обучения\n",
    "            lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-9,\n",
    "                                                               mode='min')\n",
    "            # Кастомный коллбэк для отображения скора по целевой метрике\n",
    "            metric_callback = CustomCallback(val_dataset, val_targets)\n",
    "            history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, verbose=verbose,\n",
    "                                shuffle=True, callbacks=[early_stopping_callback, lr_callback, metric_callback],\n",
    "                                workers=-1)\n",
    "            return history\n",
    "    else:\n",
    "        print(f\"Начинаю обучение модели (СPU) количество эпох = {epochs}\")\n",
    "        # Коллбэк для остановки, если модель перестала обучаться\n",
    "        early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=2.5e-6, patience=100,\n",
    "                                                                   restore_best_weights=True, mode='min')\n",
    "        # Коллбэк для уменьшения скорости обучения\n",
    "        lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-9,\n",
    "                                                           mode='min')\n",
    "        # Кастомный коллбэк для отображения скора по целевой метрике\n",
    "        metric_callback = CustomCallback(val_dataset, val_targets)\n",
    "        history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, verbose=verbose, shuffle=True,\n",
    "                            callbacks=[early_stopping_callback, lr_callback, metric_callback], workers=-1)\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Реализация кастомной функции потерь для обучения\n",
    "def tf_custom_loss(y_true, y_pred):\n",
    "    threshold = 0.6\n",
    "    error = tf.abs(y_true - y_pred) / y_true\n",
    "    is_small_error = error <= threshold\n",
    "    small_error_loss = tf.square(error / 0.15 - 1)\n",
    "    big_error_loss = 9.0 * tf.ones_like(small_error_loss) + tf.abs(error)\n",
    "    # big_error_loss = (3.0 * tf.ones_like(small_error_loss) + tf.abs(error)) ** 2\n",
    "    return tf.where(is_small_error, small_error_loss, big_error_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Компиляция текущей модели\n",
    "def compile_model(train_dataset, val_dataset, num_features, max_realty, max_region, max_city, lr=5e-4):\n",
    "    reset_tensorflow_session()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    model_input_layer = tf.keras.Input(shape=(num_features), name=\"model_features_input\")\n",
    "    model_input_realty = tf.keras.Input(shape=(1), name=\"model_realty_input\")\n",
    "    model_input_region = tf.keras.Input(shape=(1), name=\"model_region_input\")\n",
    "    model_input_city = tf.keras.Input(shape=(1), name=\"model_city_input\")\n",
    "\n",
    "    model_embedding_layer_realty = keras.layers.Embedding(max_realty + 1, 4, input_length=1, dtype=tf.float64)(\n",
    "        model_input_realty)\n",
    "    model_embedding_layer_region = keras.layers.Embedding(max_region + 1, 32, input_length=1, dtype=tf.float64)(\n",
    "        model_input_region)\n",
    "    model_embedding_layer_city = keras.layers.Embedding(max_city + 1, 32, input_length=1, dtype=tf.float64)(\n",
    "        model_input_city)\n",
    "\n",
    "    concatenated_input_layer = Concatenate()(\n",
    "        [Flatten()(model_embedding_layer_realty), Flatten()(model_embedding_layer_region),\n",
    "         Flatten()(model_embedding_layer_city), Flatten()(model_input_layer)])\n",
    "\n",
    "    layer_0 = keras.layers.Dense(128, activation=\"relu\")(concatenated_input_layer)\n",
    "    layer_1 = keras.layers.Dense(64, activation=\"relu\")(layer_0)\n",
    "    layer_2 = keras.layers.Dense(32, activation=\"relu\")(layer_1)\n",
    "    model_output_layer = keras.layers.Dense(1, activation=\"relu\", name=\"model_output\")(layer_2)\n",
    "\n",
    "    cur_model = keras.Model(\n",
    "        inputs=[\n",
    "            model_input_layer,\n",
    "            model_input_realty,\n",
    "            model_input_region,\n",
    "            model_input_city,\n",
    "        ],\n",
    "        outputs=[\n",
    "            model_output_layer,\n",
    "        ])\n",
    "\n",
    "    print(f\"Модель: input_shape = {cur_model.input_shape} output_shape = {cur_model.output_shape}\")\n",
    "#     cur_model.compile(loss=tf_custom_loss, optimizer=optimizer)  # , run_eagerly=True)\n",
    "    cur_model.compile(loss=tf_custom_loss, optimizer=optimizer)  # , run_eagerly=True)\n",
    "\n",
    "    #\n",
    "    return cur_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde69c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns_order = get_columns_order(train.columns.values.tolist())\n",
    "split_list = get_standart_split(train, n_splits=20)\n",
    "\n",
    "start_train_model_time = time.time()\n",
    "# Размер батча для Dataset\n",
    "BATCH_SIZE = int(2 ** 5)\n",
    "# Количество эпох обучения\n",
    "EPOCHS = 500\n",
    "# Количество численных входных переменных модели\n",
    "NUM_FEATURES = len(NUM_FEATURES_COLUMNS)\n",
    "# Макс. значения категориалных фичей\n",
    "MAX_REALTY = max(train['realty_type'].max(), test['realty_type'].max())\n",
    "MAX_REGION = max(train['region'].max(), test['region'].max())\n",
    "MAX_CITY = max(train['city'].max(), test['city'].max())\n",
    "# Коэффициент домножения таргета, с целью быстрейшего сходимости модельки и лучшего обучения\n",
    "MUL_TARGET = 5e-5\n",
    "\n",
    "scores = []\n",
    "nn_predicts = np.zeros(len(train))\n",
    "models_nn = []\n",
    "\n",
    "for fold_num, (train_indexes, valid_indexes) in enumerate(split_list):\n",
    "    start_time = time.time()\n",
    "    print(f\"Фолд: {fold_num}\")\n",
    "\n",
    "    train_sub_df = train[features_columns_order].loc[train_indexes].reset_index(drop=True)\n",
    "    valid_sub_df = train[features_columns_order].loc[valid_indexes].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Размер трейна = {train_sub_df.shape} Размер валидации = {valid_sub_df.shape}\")\n",
    "\n",
    "    # Строим датасеты\n",
    "    train_ds = get_dataset(\n",
    "        train_sub_df[NUM_FEATURES_COLUMNS].values,\n",
    "        train_sub_df[TARGET_COLUMNS].values * MUL_TARGET,\n",
    "        train_sub_df[['region']].values,\n",
    "        train_sub_df[['city']].values,\n",
    "        train_sub_df[['realty_type']].values,\n",
    "        BATCH_SIZE)\n",
    "    valid_ds = get_dataset(\n",
    "        valid_sub_df[NUM_FEATURES_COLUMNS].values,\n",
    "        valid_sub_df[TARGET_COLUMNS].values * MUL_TARGET,\n",
    "        valid_sub_df[['region']].values,\n",
    "        valid_sub_df[['city']].values,\n",
    "        valid_sub_df[['realty_type']].values,\n",
    "        len(valid_sub_df))\n",
    "\n",
    "    # Компилируем модель\n",
    "    model = compile_model(train_ds, valid_ds, NUM_FEATURES, MAX_REALTY, MAX_REGION, MAX_CITY)\n",
    "    # Обучаем модель\n",
    "    fit(model, EPOCHS, train_ds, valid_ds, valid_sub_df[TARGET_COLUMNS].values * MUL_TARGET)\n",
    "\n",
    "    predict_on_validation = model.predict(valid_ds)[:, 0] / MUL_TARGET\n",
    "    nn_predicts[valid_indexes] = predict_on_validation\n",
    "    targets_for_validation = valid_sub_df[TARGET_COLUMNS].values[:, 0]\n",
    "    current_score = deviation_metric(targets_for_validation, predict_on_validation)\n",
    "    scores += [current_score]\n",
    "    models_nn += [model]\n",
    "    print(\n",
    "        f\"Скор для фолда({fold_num}) : {np.round(current_score, 4)} средний скор на префиксе = {np.round(np.mean(scores), 4)} это заняло = {int(time.time() - start_time)} сек.\")\n",
    "print(f\"Процесс обучения модели занял = {int(time.time() - start_train_model_time)} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e3481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предикт нейронной сетью на test\n",
    "def get_nn_predict(models, test):\n",
    "    result = np.zeros(len(test))\n",
    "    test_ds = get_dataset(\n",
    "        test[NUM_FEATURES_COLUMNS].values,\n",
    "        np.zeros(len(test)),\n",
    "        test[['region']].values,\n",
    "        test[['city']].values,\n",
    "        test[['realty_type']].values,\n",
    "        len(test))\n",
    "    for model in models:\n",
    "        predict = model.predict(test_ds)[:, 0]\n",
    "        result += (predict / MUL_TARGET) / len(models)\n",
    "    return result\n",
    "\n",
    "\n",
    "test_nn_predict = get_nn_predict(models_nn, test)\n",
    "\n",
    "test_submission = pd.read_csv('dataset/test_submission.csv')\n",
    "\n",
    "test_submission['per_square_meter_price'] = test_nn_predict\n",
    "test_submission.to_csv('nn2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d2563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300df005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6902a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60901252",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LightGBM кастомная метрика\n",
    "def feval_deviation(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'deviation_error', deviation_metric(np.exp(y_true), np.exp(y_pred)), False\n",
    "\n",
    "\n",
    "# Функция для обучения модели LightGBM\n",
    "def train_lgb(train, valid, num_features, categorical_features, target_train, target_valid, EPOCHS, params):\n",
    "    # feature_importances = np.zeros(len(features))\n",
    "    train_dataset = lgb.Dataset(train[num_features + categorical_features], np.log(target_train), \n",
    "                                categorical_feature=categorical_features)\n",
    "    valid_dataset = lgb.Dataset(valid[num_features + categorical_features], np.log(target_valid), \n",
    "                                categorical_feature=categorical_features)\n",
    "    model = lgb.train(\n",
    "        params=params,\n",
    "        num_boost_round=EPOCHS,\n",
    "        train_set=train_dataset,\n",
    "        valid_sets=[train_dataset, valid_dataset],\n",
    "        verbose_eval=100,\n",
    "        early_stopping_rounds=int(5 / params['learning_rate']),\n",
    "        feval=feval_deviation)\n",
    "\n",
    "    y_valid = model.predict(valid[num_features + categorical_features])\n",
    "    # feature_importances = model.feature_importance(importance_type='gain') / 5.0\n",
    "    # lgb.plot_importance(model,max_num_features = 41)\n",
    "\n",
    "    return model, np.exp(y_valid)\n",
    "\n",
    "\n",
    "start_train_model_time = time.time()\n",
    "\n",
    "boosting_seed = 41\n",
    "boosting_params = {\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 1,\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.9,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.05,\n",
    "    'metric': 'custom',\n",
    "    'objective': 'regression_l1',\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': boosting_seed,\n",
    "    'feature_fraction_seed': boosting_seed,\n",
    "    'bagging_seed': boosting_seed,\n",
    "    'drop_seed': boosting_seed,\n",
    "    'data_random_seed': boosting_seed,\n",
    "}\n",
    "\n",
    "# Количество эпох обучения\n",
    "EPOCHS = 10000\n",
    "scores = []\n",
    "lgb_predicts = np.zeros(len(train))\n",
    "\n",
    "lgb_models = []\n",
    "for fold_num, (train_indexes, valid_indexes) in enumerate(split_list):\n",
    "    start_time = time.time()\n",
    "    print(f\"Фолд: {fold_num}\")\n",
    "\n",
    "    train_sub_df = train[features_columns_order].loc[train_indexes].reset_index(drop=True)\n",
    "    valid_sub_df = train[features_columns_order].loc[valid_indexes].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Размер трейна = {train_sub_df.shape} Размер валидации = {valid_sub_df.shape}\")\n",
    "    # Обучаем LightGBM и делаем предикт на валидационной выборке\n",
    "    model, predict_validation = train_lgb(\n",
    "        train_sub_df,\n",
    "        valid_sub_df,\n",
    "        NUM_FEATURES_COLUMNS,\n",
    "        CATEGORICAL_FEATURES_COLUMNS,\n",
    "        train_sub_df[TARGET_COLUMNS[0]].values,\n",
    "        valid_sub_df[TARGET_COLUMNS[0]].values,\n",
    "        EPOCHS,\n",
    "        boosting_params)\n",
    "\n",
    "    lgb_models += [model]\n",
    "    predict_on_validation = model.predict(valid_sub_df[NUM_FEATURES_COLUMNS + CATEGORICAL_FEATURES_COLUMNS])\n",
    "    lgb_predicts[valid_indexes] = np.exp(predict_on_validation)\n",
    "    targets_for_validation = valid_sub_df[TARGET_COLUMNS].values[:, 0]\n",
    "    current_score = deviation_metric(targets_for_validation, predict_on_validation)\n",
    "    scores += [current_score]\n",
    "    print(\n",
    "        f\"Скор для фолда({fold_num}) : {np.round(current_score, 4)} средний скор на префиксе = {np.round(np.mean(scores), 4)} это заняло = {int(time.time() - start_time)} сек.\")\n",
    "print(f\"Процесс обучения модели занял = {int(time.time() - start_train_model_time)} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b507f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Предикт lgb на test\n",
    "def get_lgb_predict(models, test):\n",
    "    result = np.zeros(len(test))\n",
    "    for model in models:\n",
    "        predict = model.predict(test[NUM_FEATURES_COLUMNS + CATEGORICAL_FEATURES_COLUMNS])\n",
    "        result += np.exp(predict) / len(models)\n",
    "    return result\n",
    "\n",
    "\n",
    "test_lgb_predict = get_lgb_predict(lgb_models, test)\n",
    "\n",
    "test_lgb_predict.min(), test_lgb_predict.max(), test_lgb_predict.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723e7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690e234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae0cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Кастомная метрика для xgboost\n",
    "def xbg_error(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    err = deviation_metric(np.exp(labels), np.exp(preds))\n",
    "    return 'deviation_error', err\n",
    "\n",
    "\n",
    "def train_xgb(train, valid, num_features, categorical_features, target_train, target_valid, EPOCHS, params):\n",
    "    dtest = xgb.DMatrix(test[num_features + categorical_features])\n",
    "    y_valid = np.zeros(len(valid))\n",
    "\n",
    "    dtrain = xgb.DMatrix(train[num_features + categorical_features], np.log(target_train), \n",
    "                        )\n",
    "    dvalid = xgb.DMatrix(valid[num_features + categorical_features], np.log(target_valid), \n",
    "                        )\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        EPOCHS,\n",
    "        [(dvalid, \"valid\")],\n",
    "        verbose_eval=250,\n",
    "        early_stopping_rounds=500,\n",
    "        feval=xbg_error,\n",
    "    )\n",
    "    y_valid = model.predict(dvalid)\n",
    "\n",
    "    return model, y_valid\n",
    "\n",
    "\n",
    "start_train_model_time = time.time()\n",
    "\n",
    "xgboost_seed = 41\n",
    "xgboost_params = {\n",
    "    \"subsample\": 0.60,\n",
    "    \"colsample_bytree\": 0.40,\n",
    "    \"max_depth\": 7,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    'disable_default_eval_metric': 1,\n",
    "    \"nthread\": -1,\n",
    "    \"max_bin\": 64,\n",
    "    'min_child_weight': 0.0,\n",
    "    'reg_lambda': 0.0,\n",
    "    'reg_alpha': 0.0,\n",
    "    'seed': xgboost_seed,\n",
    "}\n",
    "\n",
    "# Количество эпох обучения\n",
    "EPOCHS = 10000\n",
    "scores = []\n",
    "xgb_predicts = np.zeros(len(train))\n",
    "\n",
    "xgb_models = []\n",
    "for fold_num, (train_indexes, valid_indexes) in enumerate(split_list):\n",
    "    start_time = time.time()\n",
    "    print(f\"Фолд: {fold_num}\")\n",
    "\n",
    "    train_sub_df = train[features_columns_order].loc[train_indexes].reset_index(drop=True)\n",
    "    valid_sub_df = train[features_columns_order].loc[valid_indexes].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Размер трейна = {train_sub_df.shape} Размер валидации = {valid_sub_df.shape}\")\n",
    "    # Обучаем Xgboost и делаем предикт на валидационной выборке\n",
    "    model, predict_validation = train_xgb(\n",
    "        train_sub_df,\n",
    "        valid_sub_df,\n",
    "        NUM_FEATURES_COLUMNS,\n",
    "        CATEGORICAL_FEATURES_COLUMNS,\n",
    "        train_sub_df[TARGET_COLUMNS[0]].values,\n",
    "        valid_sub_df[TARGET_COLUMNS[0]].values,\n",
    "        EPOCHS,\n",
    "        xgboost_params)\n",
    "\n",
    "    xgb_models += [model]\n",
    "    predict_on_validation = model.predict(\n",
    "        xgb.DMatrix(valid_sub_df[NUM_FEATURES_COLUMNS + CATEGORICAL_FEATURES_COLUMNS]))\n",
    "    xgb_predicts[valid_indexes] = np.exp(predict_on_validation)\n",
    "    targets_for_validation = valid_sub_df[TARGET_COLUMNS].values[:, 0]\n",
    "    current_score = deviation_metric(targets_for_validation, predict_on_validation)\n",
    "    scores += [current_score]\n",
    "    print(\n",
    "        f\"Скор для фолда({fold_num}) : {np.round(current_score, 4)} средний скор на префиксе = {np.round(np.mean(scores), 4)} это заняло = {int(time.time() - start_time)} сек.\")\n",
    "print(f\"Процесс обучения модели занял = {int(time.time() - start_train_model_time)} секунд\")\n",
    "\n",
    "\n",
    "# Предикт xgb на test\n",
    "def get_xgb_predict(models, test):\n",
    "    result = np.zeros(len(test))\n",
    "    for model in models:\n",
    "        predict = model.predict(xgb.DMatrix(test[NUM_FEATURES_COLUMNS + CATEGORICAL_FEATURES_COLUMNS]))\n",
    "        result += predict / len(models)\n",
    "    return result\n",
    "\n",
    "\n",
    "test_xgb_predict = get_xgb_predict(xgb_models, test)\n",
    "\n",
    "test_xgb_predict.min(), test_xgb_predict.max(), test_xgb_predict.mean()\n",
    "\n",
    "train_targets = train[TARGET_COLUMNS[0]].values\n",
    "\n",
    "\n",
    "def minimize_arit(W):\n",
    "    ypred = W[0] * nn_predicts + W[1] * lgb_predicts + W[2] * xgb_predicts\n",
    "    return deviation_metric(train_targets, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostEvalMetricPearson(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return error\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return False\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "        preds = np.array(approxes[0])\n",
    "        target = np.array(target)\n",
    "        err = deviation_metric(np.exp(target), np.exp(preds))\n",
    "        return err, 0\n",
    "\n",
    "\n",
    "def train_cat(train, valid, num_features, categorical_features, target_train, target_valid, EPOCHS):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_data = Pool(data=test[num_features + categorical_features],\n",
    "                  cat_features=categorical_features)\n",
    "\n",
    "\n",
    "    train_data = Pool(data=train[num_features + categorical_features],\n",
    "                      cat_features=categorical_features,\n",
    "                      label=np.log(target_train))\n",
    "\n",
    "    val_data = Pool(data=valid[num_features + categorical_features],\n",
    "                cat_features=categorical_features,\n",
    "                  label=np.log(target_valid))\n",
    "    \n",
    "\n",
    "    cat_model = CatBoostRegressor(\n",
    "        learning_rate=0.012,\n",
    "        iterations=15000,\n",
    "        metric_period=50,\n",
    "        eval_metric=CatBoostEvalMetricPearson(),\n",
    "    )\n",
    "    cat_model.fit(train_data, eval_set=val_data, use_best_model=True, early_stopping_rounds=300)\n",
    "  \n",
    "    y_valid = cat_model.predict(test_data)\n",
    "\n",
    "    return cat_model, y_valid\n",
    "\n",
    "\n",
    "start_train_model_time = time.time()\n",
    "\n",
    "scores = []\n",
    "cat_predicts = np.zeros(len(train))\n",
    "\n",
    "cat_models = []\n",
    "for fold_num, (train_indexes, valid_indexes) in enumerate(split_list):\n",
    "    start_time = time.time()\n",
    "    print(f\"Фолд: {fold_num}\")\n",
    "\n",
    "    train_sub_df = train[features_columns_order].loc[train_indexes].reset_index(drop=True)\n",
    "    valid_sub_df = train[features_columns_order].loc[valid_indexes].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Размер трейна = {train_sub_df.shape} Размер валидации = {valid_sub_df.shape}\")\n",
    "    model, predict_validation = train_cat(\n",
    "        train_sub_df,\n",
    "        valid_sub_df,\n",
    "        NUM_FEATURES_COLUMNS,\n",
    "        CATEGORICAL_FEATURES_COLUMNS,\n",
    "        train_sub_df[TARGET_COLUMNS[0]].values,\n",
    "        valid_sub_df[TARGET_COLUMNS[0]].values,\n",
    "        EPOCHS\n",
    "        )\n",
    "\n",
    "    cat_models += [model]\n",
    "    predict_on_validation = model.predict(valid_sub_df[NUM_FEATURES_COLUMNS + CATEGORICAL_FEATURES_COLUMNS])\n",
    "    cat_predicts[valid_indexes] = np.exp(predict_on_validation)\n",
    "    targets_for_validation = valid_sub_df[TARGET_COLUMNS].values[:, 0]\n",
    "    current_score = deviation_metric(targets_for_validation, predict_on_validation)\n",
    "    scores += [current_score]\n",
    "    print(\n",
    "        f\"Скор для фолда({fold_num}) : {np.round(current_score, 4)} средний скор на префиксе = {np.round(np.mean(scores), 4)} это заняло = {int(time.time() - start_time)} сек.\")\n",
    "print(f\"Процесс обучения модели занял = {int(time.time() - start_train_model_time)} секунд\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02607e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Предикт xgb на test\n",
    "def get_cat_predict(models, test):\n",
    "    result = np.zeros(len(test))\n",
    "    for model in cat_models:\n",
    "        predict = model.predict(test[NUM_FEATURES_COLUMNS + CATEGORICAL_FEATURES_COLUMNS])\n",
    "        result += np.exp(predict) / len(models)\n",
    "    return result\n",
    "\n",
    "\n",
    "test_cat_predict = get_cat_predict(xgb_models, test)\n",
    "\n",
    "test_cat_predict.min(), test_cat_predict.max(), test_cat_predict.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45587c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_arit(W):\n",
    "    ypred = W[0] * nn_predicts + W[1] * lgb_predicts + W[2] * xgb_predicts + W[3] * cat_predicts\n",
    "    return deviation_metric(train_targets, ypred)\n",
    "\n",
    "\n",
    "W = minimize(minimize_arit, [1.0 / 4] * 4, options={'gtol': 1e-6, 'disp': True}).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677681d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_submission = pd.read_csv('dataset/test_submission.csv')\n",
    "test_submission['per_square_meter_price'] = test_nn_predict * W[0] + test_lgb_predict * W[1] + np.exp(test_xgb_predict) * W[2] + test_cat_predict * W[3]\n",
    "test_submission['per_square_meter_price'] = test_submission['per_square_meter_price'].apply(lambda x: max(0.0, x))\n",
    "test_submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d759994",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xgb_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c2a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
